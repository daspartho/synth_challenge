{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNt+jZ9n5nLzy1ZmBzZAo9G"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "HirBIOfnRfC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### install required libraries"
      ],
      "metadata": {
        "id": "UczRjDkHZE7i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmBN1XYwUQEl"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers -U\n",
        "!pip install accelerate -U\n",
        "!pip install librosa\n",
        "!pip install evaluate -U\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFace Hub login\n",
        "\n",
        "for common voice dataset access"
      ],
      "metadata": {
        "id": "XoVvlGrPZAcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "4iW3iz0PZJZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Dataset"
      ],
      "metadata": {
        "id": "0NXNQ5cHSH5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll get the hindi subset of the Fleurs dataset and the Common Voice 13 dataset (more \"Hindi\" data)"
      ],
      "metadata": {
        "id": "LxLWyRtbXYZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "fleurs = DatasetDict()\n",
        "fleurs[\"train\"] = load_dataset(\"google/fleurs\", \"hi_in\", split=\"train\")\n",
        "fleurs[\"valid\"] = load_dataset(\"google/fleurs\", \"hi_in\", split=\"validation\")\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"hi\", split=\"train\")\n",
        "common_voice[\"valid\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"hi\", split=\"validation\")\n",
        "\n",
        "print(fleurs)\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "YOa3O9tRSqDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing additional metadata information which we don't need"
      ],
      "metadata": {
        "id": "KKdoNmFhcpOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs = fleurs.remove_columns(['id', 'num_samples', 'path', 'raw_transcription', 'gender', 'lang_id', 'language', 'lang_group_id'])\n",
        "common_voice = common_voice.remove_columns(['client_id', 'path', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'])\n",
        "print(fleurs)\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "PFnbhBwmVFB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "combining both datasets"
      ],
      "metadata": {
        "id": "HiVIxpBf37OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets, Audio\n",
        "\n",
        "# rename 'sentence' column in common voice dataset to align with fleurs dataset\n",
        "common_voice = common_voice.rename_column('sentence', 'transcription')\n",
        "\n",
        "# downsample audio in common voice dataset to align sampling rate with fleurs dataset\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "# now we combine\n",
        "ds = DatasetDict()\n",
        "ds['train'] = concatenate_datasets([fleurs['train'], common_voice['train']])\n",
        "ds['valid'] = concatenate_datasets([fleurs['valid'], common_voice['valid']])\n",
        "\n",
        "ds"
      ],
      "metadata": {
        "id": "o0kvn9Rv28DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a WhisperProcessor\n",
        "\n",
        "It combines both whisper feature extractor and tokenizer.\n",
        "\n",
        "Feature Extractor to pre-processes the raw audio inputs by padding/truncating them to length of 30s and then converting them to log-Mel spectrograms.\n",
        "\n",
        "Tokenizer to post-processes the model outputs (index of predicted text) to text format."
      ],
      "metadata": {
        "id": "kfNsqi7HpN66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "model_checkpoint=\"openai/whisper-base\"\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(model_checkpoint, language=\"Hindi\", task=\"transcribe\") # these arguments specifies the tokenizer to append the language token and the task token to the start of the sequence"
      ],
      "metadata": {
        "id": "qqHLkMfgo2OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset for training"
      ],
      "metadata": {
        "id": "f9-Pr3fDqYs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset preparation function\n",
        "def prepare_dataset(batch):\n",
        "\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array using the feature extractor\n",
        "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids using the tokenizer\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"transcription\"]).input_ids\n",
        "    return batch\n"
      ],
      "metadata": {
        "id": "2RyNOklvqdbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the dataset preparation function to all training examples\n",
        "ds = ds.map(prepare_dataset, remove_columns=ds.column_names[\"train\"])\n",
        "ds"
      ],
      "metadata": {
        "id": "AYwQLGz9qs6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a data collator\n",
        "to take our pre-processed data and convert them to PyTorch tensors."
      ],
      "metadata": {
        "id": "dInUBxaR-Wg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "\n",
        "        # get the log-Mel input features\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        # no padding is applied as they are already padded, it is to convert to pytorch tensors\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length and convert to pytorch tensors\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding tokens with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "VlBhoL0I9lQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "initialise the data collator"
      ],
      "metadata": {
        "id": "PV38_ICSCSdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "KbEwM5eY9qmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation metrics\n",
        "\n",
        "we need to define a `compute_metrics` function to evaluate the model using the WER metric."
      ],
      "metadata": {
        "id": "S-eFn_iR-kif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll load the WER metric from evaluate library\n",
        "\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "RPaess2l9rOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    # (undoing the step we applied in the data collator to ignore padded tokens correctly in the loss)\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # decodes the predicted and label ids to strings\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # computes the WER between the predictions and reference labels\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n"
      ],
      "metadata": {
        "id": "Bi0K33479tNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a pre-trained checkpoint\n",
        "\n",
        "we need to load a pre-trained checkpoint and configure it correctly for training.\n",
        "\n"
      ],
      "metadata": {
        "id": "SCVEYxQt-qs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "LlfTuJdI90mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll train the model to predict the correct language and task instead of having forced token ids control it\n",
        "model.config.forced_decoder_ids = None\n",
        "# there are some tokens that are completely supressed during generation, we'll disable that\n",
        "model.config.suppress_tokens = []"
      ],
      "metadata": {
        "id": "SelD9K0k92aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the training arguments"
      ],
      "metadata": {
        "id": "wm8qCpQx-xt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "bs=32\n",
        "epochs=10\n",
        "lr=1e-5\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-base-hindi\",\n",
        "    per_device_train_batch_size=bs,\n",
        "    per_device_eval_batch_size=bs*2,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    num_train_epochs=epochs,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=lr,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_ratio=0.1,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    seed=42,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "q4wC2Qal93B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train\n",
        "\n",
        " forward the training arguments to the trainer along with our model, dataset, data collator and `compute_metrics` function"
      ],
      "metadata": {
        "id": "CqjF4pzh-1f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"valid\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "faCxfubd99zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And train"
      ],
      "metadata": {
        "id": "hQN3b3mGH9_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "3-sTSlRr9-Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push the model to Hub"
      ],
      "metadata": {
        "id": "kpsFFTHCLoQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "5hk_GblFIUYf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}